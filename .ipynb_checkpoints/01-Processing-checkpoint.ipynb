{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Processing\n",
    "In this notebook, we will be loading the data necessary for this project, combining the data sources and performing initial data processing steps, in order to prepare the dataset for visualization and additional data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.util.paths import DATA_PATH\n",
    "from lib.processing import load_from_pkl, save_to_pkl\n",
    "from lib.pipelines import extracted_pipe, api_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this project has been sourced from Discogs.com, the largest online marketplace for second-hand physical music. There are three components to the data that will make up our final dataset, namely:\n",
    "\n",
    "1. ``api_df`` -> Data taken from the official Discogs.com API (https://www.discogs.com/developers)\n",
    "2. ``extracted_df`` -> Data scraped from Discogs release pages\n",
    "3. ``high_level_features_df`` -> High-Level Features extracted from the cover images of each Album\n",
    "\n",
    "At this juncture, we will load, investigate and transform the first two data sources outlined above. The data was collected continuously over the months of February and March 2020 using the scripts found in the data directory of this project. To scrape each of these three data sources, the main() function of the data/main.py file was used. It is not recommended to run this script oneself, as it will require substantial time to complete and potentially the investment into a proxy service provider. A sample of all data used in this project has been made available for the EPFL Extension School Reviewers in the ``data/sample_data`` directory, including the DataFrames used as the outset for our processing here, as well as the final composite data that we generate in notebook ``03-DatasetPrep``. For more details on how each data source was filtered and scraped, please refer to the classes and functions contained in the ``data/util/scrape.py`` module.\n",
    "    \n",
    "First, we will load the data extracted from the Discogs.com API, henceforth referred to as ``api_df``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Data\n",
    "### Loading and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df = load_from_pkl('api',path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see the data saved from the querying of the Discogs.com API. The data includes all available jazz albums from the API and retains the columns that were deemed to be the most relevant and informative for the purposes of estimating the market value of each record. As we can see from the above calling of the ``.head()`` method on the api_data DataFrame, several columns have been saved in a binary format. This is due to the fact that the API often returned multiple categories for these columns, and as such they were initially saved in a list in the process of scraping. As this data was originally stored in a SQL database, it has been pickled to enable the storage of the data. Let us now unpickle this data in order to get a better understanding of the structure of the ``api_data``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df.applymap(lambda x: pickle.loads(x) if isinstance(x,bytes) else x).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all columns are now human-readable, we can discuss each column and its relevance for the purposes of our goal of estimating the market value of jazz albums on the basis of their meta-characteristics\n",
    "- ``release_id`` and ``master_id``\n",
    "    - These columns represent the unique identifier of a specific release of a record on Discogs.com, and the unique identifier of the album irrespective of its release, respectively.\n",
    "    - While ``release_id`` will be of no use to us in the estimation of album market value, it is valuable as a primary key for the joining of the data extracted from the Discogs.com API with the scraped release page and image data we will be introducing later\n",
    "    - ``master_id`` will be useful in constructing linkages between entries that are the same album, but released in a different country, format, or by another label\n",
    "- ``country``\n",
    "    - This feature gives us an insight into which country the record was released from\n",
    "    - While not immediately obvious from the snapshot above, this feature can have multiple countries for its value, such as \"England & USA\", which requires a more thoughtful approach than just direct one-hot encoding in order to optimally preserve the information it encodes\n",
    "- ``title``\n",
    "    - This feature includes both the title of the album, as well as the title of the artist\n",
    "    - It will be necessary to split this feature into two, such that connections between albums can be drawn on the basis of being authored by the same artist or group\n",
    "- ``community_want`` and ``community_have``\n",
    "    - These features outline the registered demand and supply, respectively, for albums on the Discogs.com platform\n",
    "    - Users can register their ownership or desire of a certain album, which is then aggregated over users and recorded in these features\n",
    "    - These are the first features we see which will not be taken into account in the 'Record Store Scenario', as in a physical visit to the record store, it is not possible to know exactly how many people have and want a specific album\n",
    "- ``genre``, ``style`` and ``label``\n",
    "    - These features outline the genres, styles and labels associated with each album release\n",
    "    - As has been shown above, there are potentially multiple values for this feature, which will make standard encoding methods such as One-Hot Encoding difficult to rationalize\n",
    "- ``formats``\n",
    "    - Taken directly from the Discogs.com API, the formats feature is the most unruly of all, as it contains up to 4 sub-features, namely:\n",
    "        - Format Description\n",
    "            - Release-specific information related to its format\n",
    "            - Examples\n",
    "                - '10\"', '78RPM', 'Album', 'Reissue'\n",
    "        - Format Text\n",
    "            - Additional free form notes associated with the record release\n",
    "            - Examples\n",
    "                - 'Paper Sleeve', 'Red Vinyl'\n",
    "        - Format Name\n",
    "            - The name of the format\n",
    "            - Examples\n",
    "                - 'CD', 'Vinyl', 'Cassette'\n",
    "        - Format Quantity\n",
    "            - An integer representing how many units of the format are associated with a purchase of the release\n",
    "- ``thumb_url`` and ``release_url``\n",
    "    - These features are unnecessary for the purposes of our analysis here, but were useful in order to conduct the data extraction of the ``extracted_df`` and ``image_embedding_df`` obtained from the Discogs.com platform itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having taken an initial look at ``api_df``, we can now move on to the cleaning and transformation of this DataFrame through a pre-written pipeline, which is to be found in the ``lib/pipelines.py`` module. For the purposes of exposition, the pipeline will be replicated here step-by-step, with motivation and explanation for each step. In the following notebooks, a direct import of the pipeline will be used in lieu of a replication as below.\n",
    "\n",
    "Below the required transformers will be imported and composed into the pre-defined pipeline structure, after which the pipeline will be explained in its entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from lib.transformers import (ColumnRemover, \n",
    "                              TitleSplitter, \n",
    "                              Unpickler, \n",
    "                              LabelCleanReduce, \n",
    "                              ArtistCleanReduce, \n",
    "                              DuplicateRemover, \n",
    "                              CountryEncoder, \n",
    "                              GenreEncoder, \n",
    "                              MultiValueCategoricalEncoder, \n",
    "                              FormatEncoder, \n",
    "                              FormatTextCleanReduce, \n",
    "                              TimePeriodEncoder,\n",
    "                              OutlierRemover,\n",
    "                              DummyGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_pipe = Pipeline([\n",
    "    #Step 4a - Homogenize and clean the label feature\n",
    "    ('label',LabelCleanReduce()),\n",
    "    #Step 4b - Homogenize and clean the artist feature\n",
    "    ('artist',ArtistCleanReduce())\n",
    "])\n",
    "\n",
    "column_encoding_pipe = Pipeline([\n",
    "    #Step 6a - Identify regions and superregions associated with countries and approrpiately encode\n",
    "    #multi-country album releases\n",
    "    ('country',CountryEncoder()),\n",
    "    #Step 6b - Expand and encode the genre feature\n",
    "    ('genre',GenreEncoder()),\n",
    "    #Step 6c - Expand and encode the style feature\n",
    "    ('style',MultiValueCategoricalEncoder(feature='style')),\n",
    "    #Step 6d - OneHotEncode 'format_name', while maintaining the DataFrame structure\n",
    "    ('encode_format_name', DummyGenerator('format_name'))\n",
    "])\n",
    "\n",
    "format_pipe = Pipeline([\n",
    "    #Step 7a - Create the format_description, format_text, format_name and format_quantity features\n",
    "    ('make_columns',FormatEncoder()),\n",
    "    #Step 7b - Remove format_quantity outliers\n",
    "    ('remove_quantity_outliers', OutlierRemover('format_quantity')),\n",
    "    #Step 7c - Expand and encode the format_description feature\n",
    "    ('encode_descriptions',MultiValueCategoricalEncoder('format_description')),\n",
    "    #Step 7d - Homogenize and clean the format_text feature\n",
    "    ('clean_format_text',FormatTextCleanReduce())\n",
    "])\n",
    "\n",
    "api_pipe = Pipeline([\n",
    "    #Step 1 - Remove Unnecessary Columns\n",
    "    ('remove_columns',ColumnRemover(['id','thumb_url','release_url'])),\n",
    "    #Step 2 - Splitting title Feature into title and artist\n",
    "    ('split_title',TitleSplitter()),\n",
    "    #Step 3 - Unpickle Pickled Features\n",
    "    ('unpickle',Unpickler(['genre','style','label','formats'])),\n",
    "    #Step 4 - Clean Features with Multiple Categorical Values per Sample and High Categorical Variance\n",
    "    ('clean_text',clean_text_pipe),\n",
    "    #Step 5 - Remove Entries with Duplicate release_ids\n",
    "    ('remove_duplicates',DuplicateRemover('release_id')),\n",
    "    #Step 6 - Encode Categorical Features which cannot be encoded conventionally\n",
    "    ('encode_columns',column_encoding_pipe),\n",
    "    #Step 7 - Expand the 'format' Feature from Dictionary to Column Format\n",
    "    ('format_columns',format_pipe),\n",
    "    #Step 8 - Encode Jazz Periods and Eras associated with the Release Year of each Album\n",
    "    ('encode_time_periods',TimePeriodEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Step Exposition\n",
    "#### Step 1 - Removing Unnecessary Columns\n",
    "This step is mostly self-explanatory. For the purposes of this analysis, neither ``thumb_url`` and ``release_url`` will be relevant, and the ``id`` column is superfluous.\n",
    "#### Step 2 - Splitting ``title`` Feature into ``title`` and ``artist``\n",
    "As was alluded to in the initial ``api_df`` overview, the original ``title`` feature clearly contains two distinct pieces of information, namely the name of the performing artist or group for a given release, as well as the name of said release. As such, with this transformer, we split the title feature according to the common delimiter in this column, namely a hyphen ('-') in order to yield an individual ``title`` and ``artist`` column. An equivalent operation is shown below on a sample of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df['title'].head().str.split('-',n=1,expand=True).rename(columns={0:'artist',1:'title'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Unpickle Pickled Features\n",
    "This step replicates the operation performed in the ``api_df`` overview, by unpickling ``genre``,``style``,``label`` and ``formats``\n",
    "#### Step 4 - Clean Features with Multiple Categorical Values per Sample and High Categorical Variance\n",
    "For the features ``label`` and ``artist``, it is clear that a more sophisticated approach is necessary than just simple One-Hot Encoding, as each has a high unique cardinality, and is composed of fairly dirty textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_artist_columns = api_pipe.steps[2][-1].fit_transform(api_pipe.steps[1][-1].fit_transform(api_df)).loc[:,['label','artist']]\n",
    "label_artist_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us identify the number of unique values for each of these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_values = label_artist_columns.loc[:,'label'].apply(lambda x: 'üü'.join(x)).str.split('üü',expand=True).stack()\n",
    "artist_values = label_artist_columns.loc[:,'artist']\n",
    "for values, column in zip((label_values,artist_values),('label','artist')):\n",
    "    print('The api_data dataset has %s unique %s values' % (len(np.unique(values)),column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output above, the order of magnitude of unique values for these columns prevents us from taking the standard approach in machine learning of One-Hot Encoding these columns, in the interest of preserving the dimensionality of our dataset. Let us now see if there is any possibility of reducing the number of unique values, by homogenizing similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(label_values).most_common(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can clearly see that labels have been entered under slightly different names in some cases, in particular for the  following pairs:\n",
    "- Capitol Records & Capitol Records, Inc.\n",
    "- CBS & CBS Inc.\n",
    "Unifying these would by themselves already greatly increase the number of connected entries by homogenizing the names of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(artist_values).most_common(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of artists, it is not as clear that such cleaning would be required in order to get widespread connection between records, however, by taking a look at all entries containing 'Miles Davis', for example, we quickly observe in how many varied ways Miles Davis appears in the ``artist`` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miles_davis_values = tuple(filter(lambda x: 'miles davis' in x.lower(), Counter(artist_values)))\n",
    "miles_davis_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, in order to reduce the dimensionality and increase the linkages between different jazz albums, it will also be crucial to clean the ``artist`` column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this, we apply the ``ArtistCleanReduce`` and ``LabelCleanReduce`` transformers, which clean and homogenize the text inputs, and match the resulting unique strings with one another using TF-IDF Vectorization with n-grams. In the case of ``LabelCleanReduce``, we pick the first unique element attributed to an entry, as it is deemed to be the most relevant label for the release. For both the ``artist`` and ``label`` columns, this results in a great reduction in the cardinality of unique entries associated with both columns, as we can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text_pipe.fit_transform(label_artist_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ('label','artist'):\n",
    "    print('The cleaned api_data has %s unique %s values' % (len(cleaned_text.loc[:,column].unique()),column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have successfully reduced the unique cardinality of ``label`` and ``artist``, these values still far exceed the threshold under which it would be acceptable to apply One-Hot Encoding. As such, we will use the Leave One Out approach to encode these features, which will allow us to eliminate this explosion of dimensionality. The approach will be explained in further detail in ``04-Modelling``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Namespace Cleanup\n",
    "del label_artist_columns,label_values,artist_values,miles_davis_values,cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 - Remove Entries with Duplicate ``release_ids``\n",
    "The results obtained from querying the Discogs.com API yielded a substantial number of duplicates for the ``release_id`` feature, as we can see from the cell below. As this is a unique identifier for each album, we will remove those entries for which a duplicate ``release_id`` is identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of entries with duplicated release_id values is:',len(api_df[api_df['release_id'].duplicated()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6 - Encode Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ``column_encoding_pipe``, we turn our attention to the encoding of the ``country``, ``genre`` and ``style`` columns shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = Unpickler(['genre','style']).fit_transform(api_df.loc[:,['country','genre','style']])\n",
    "categorical_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output above, we have two different encodings, as in the ``clean_text_pipe``. The ``country`` column is of a string type, whereas the ``genre`` and ``style`` features contain list-type values for each sample. It is important to know that the ``country`` attribute is provided by users themselves, whereas the ``genre`` and ``style`` values are set by Discogs and are thus more standardized.\n",
    "\n",
    "As such, for the ``genre`` and ``style`` columns, we simply need to utilize dummy encoding in order to encode the information from these features. We do this by implementing and extending the ``MultiValueCategoricalEncoder`` class which encodes the features accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.transformers import MultiValueCategoricalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_genre_style = MultiValueCategoricalEncoder('style')\\\n",
    "    .fit_transform(\n",
    "        GenreEncoder()\\\n",
    "            .fit_transform(categorical_features)\n",
    ")\n",
    "encoded_genre_style.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with the ``country`` feature, we take a different approach. As each country associated with a given sample contains information related to the broader region of the globe the album comes from, we implement a custom transformer called ``CountryEncoder``, which reads in the string input of the ``country`` feature, and dummy encodes the information according to which countries the album comes from, and additionally add columns outlining the regions and superregions the album release belongs to. In this case, regions refer to the the UN Geoschemes according to their M49 classification code. Superregions are used to refer to the following regions which supersede and contain within them the UN Geoscheme regions:\n",
    "- Africa\n",
    "- Americas\n",
    "- Asia\n",
    "- Europe\n",
    "- Oceania\n",
    "- Unknown (in case of missing information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountryEncoder().fit_transform(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del categorical_features, encoded_genre_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7 - Create and Encode Format Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we must decompose the formats column such that its component features are interpretable for the machine learning algorithms we aim to build as part of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unpickler('formats').fit_transform(api_df)['formats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``formats`` column is saved in a dictionary, which means that we must extract the value of each key of the dictionaries and create a new independent feature. We do this via the ``FormatEncoder`` transformer, which executes this for the four keys that are found throughout the feature, and creates the columns ``format_description``, ``format_text``,``format_name``, ``format_quantity``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = FormatEncoder().fit_transform(Unpickler('formats').fit_transform(api_df))[['format_name','format_quantity','format_text','format_description']]\n",
    "formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above features, we have two fairly cleanly formatted ones in ``format_name`` and ``format_quantity``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats['format_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(formats['format_quantity'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can see from the range of values in ``format_name`` that we require no additional encoding other than One-Hot encoding prior to estimation at this stage, we see that there is a fairly long tail in the quantity of units that are associated with each album. In order to avoid these outliers from negatively impacting our predictions, we use the ``OutlierRemover`` transformer which removes values that are more than three standard deviations removed from the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(OutlierRemover('format_quantity').fit_transform(formats)['format_quantity'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ``format_text``and ``format_description`` features, we utilize similar approaches as in previous sections, due to similar issues occuring in the data. For ``format_text``, we extend the ``FeatureCleanReduce`` transformer to create the ``FormatTextCleanReduce`` transformer in order to reduce the feature's cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(formats['format_text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(FormatTextCleanReduce().fit_transform(formats)['format_text'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the cardinality of the feature is still fairly high, we will forego dummy encoding at this point and use the Leave One Out encoding approach we have previously discussed emplyoing for ``artist`` and ``label`` features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ``format_description`` feature, we observe that there is a more standardized set of values for the feature, and as such we can simply apply the ``MultiValueCategoricalEncoder`` as in previous sections for features with list datatypes, such as ``genre`` and ``style``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiValueCategoricalEncoder('format_description').fit_transform(formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will One-Hot encode the ``format_name`` feature, as its structure of singular, standardized values in the dataset makes it ideal for such an encoding procedure, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats['format_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement this One-Hot encoding using the custom ``DummyGenerator`` transformer, which we use in order to be able to apply the One-Hot encoding within our transformation pipeline, without having to forgo the DataFrame format in downstream preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8 - Create and Encode Time Period Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional step of feature engineering, we will utilize the ``year`` feature to encode some information about the distinct eras taking place in the history of Jazz music. This information is obtained from the following source: https://www.preceden.com/timelines/3242-jazz-history, and encodes the following time periods and eras:\n",
    "- Eras\n",
    "    - Swing (1925 - 1945)\n",
    "    - Modern (1940 - 1970)\n",
    "    - Contemporary (1970 - 2020)\n",
    "- Periods\n",
    "    - Big Band (1930 - 1950)\n",
    "    - Bebop (1940 - 1955)\n",
    "    - Cool (1950 - 1970)\n",
    "    - Fusion (1970 - 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimePeriodEncoder().fit_transform(api_df)[[\n",
    "    'year','era_modern','era_contemporary','era_swing',\n",
    "    'period_big_band','period_bebop','period_cool'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesis\n",
    "All in all, we combine these steps through the ``api_pipe``, which yields the following output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_pipe.fit_transform(api_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracted Data\n",
    "### Loading and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df = load_from_pkl('extracted',path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unpickler('track_titles').fit_transform(extracted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``extracted_df`` DataFrame represents the data that was obtained from scraping Discogs.com release pages for the albums which were obtained through the initial querying of their API. This data is crucial in being able to build the model estimating the market value of jazz albums, as no price information is divulged by the API, and can only be programmatically obtained through web scraping. Additionally, the release pages for albums on Discogs are additionally informative in that they provide platform-specific metadata which can ultimately be used in predicting market value in the previously outlined full-information scenario. \n",
    "\n",
    "What follows is a description of each variable and its relevance to the project:\n",
    "- ``release_id``\n",
    "    - As for the ``api_data``, the release_id is the unique identifier for each album release and will form the primary key with which we can later join our data sources\n",
    "- ``market_price``\n",
    "    - Records the lowest price at which a given album release is available for on the Discogs.com platform.\n",
    "        - Releases which have no open listings on the platform have null values\n",
    "- ``units_for_sale``\n",
    "    - Records how many units of the album release are available on the Discogs.com platform\n",
    "- ``average_rating``\n",
    "    - Records the mean score out of 5 attributed to the album release by Discogs.com users\n",
    "- ``rating_count``\n",
    "    - Records the number of ratings that have been submitted for the album release\n",
    "- ``last_sold``\n",
    "    - Records the last time the album release was sold on the Discogs.com platform\n",
    "- ``number_of_tracks``\n",
    "    - Generated during scraping\n",
    "    - Records the total number of tracks that are listed for an album release\n",
    "- ``running_time``\n",
    "    - Generated during scraping\n",
    "    - Records the total ``running_time`` of the album in minutes\n",
    "- ``lowest``, ``median``, ``highest``\n",
    "    - Record the lowest, median and highest prices each album was sold for on the platform, respectively. \n",
    "- ``track_titles``\n",
    "    - Records the track titles of the ablums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Pipeline\n",
    "As for ``api_df``, we will replicate the transformation pipeline used to process the data scraped from Discogs.com release pages and explain each step of the pipeline in further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.transformers import (\n",
    "    ColumnRemover,\n",
    "    ColumnCombiner,\n",
    "    DuplicateRemover,\n",
    "    NullRemover,\n",
    "    StandardCountEncoder,\n",
    "    LastSoldEncoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_value_pipe =  Pipeline([\n",
    "    #Step 3a - Creates the market_value feature by imputing 'median' nulls with market_price values\n",
    "    ('make_market_value', ColumnCombiner('median','market_price','market_value')),\n",
    "    #Step 3b - Removes samples with null values for market_value\n",
    "    ('remove_nulls', NullRemover('market_value')),\n",
    "    #Step 3c - Removes samples with outlier values for market_value according to 3 standard deviation rule\n",
    "    ('remove_outliers', OutlierRemover('market_value'))\n",
    "])\n",
    "\n",
    "extracted_pipe = Pipeline([\n",
    "    #Step 1 - Removes unnecessary columns\n",
    "    ('remove_id', ColumnRemover(['id','last_sold'])),\n",
    "    #Step 2 - Unpickles the track_titles features\n",
    "    ('unpickle', Unpickler('track_titles')),\n",
    "    #Step 3 - Creates and processes the ``market_value`` feature\n",
    "    ('make_market_value',market_value_pipe),\n",
    "    #Step 4 - Removes any duplicates present in the release_id feature\n",
    "    ('remove_duplicates', DuplicateRemover('release_id')),\n",
    "    #Step 5 - Counts the number of Jazz standards each album has\n",
    "    ('count_standards',StandardCountEncoder('track_titles',DATA_PATH))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to the ``api_df`` DataFrame, the ``extracted_df`` DataFrame is far less unwieldy and messy, and instead requires only small adjustments to be made to the core features, as well as some additional transformations for feature engineering purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Removing unnecessary features\n",
    "We decide to remove the ``last_sold`` feature due to it having an untenable number of null values, and due to the high variation in the ranges in which albums have last been sold for the albums in question, the missing values cannot be imputed in good conscience. As such, the feature is dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df['last_sold'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Unpickling ``track_titles``\n",
    "The ``track_titles`` feature is stored as a list, and as such needed to be pickled before being saved to the SQL database that was used for the intermediate storing of data during scraping. As such, we unpickle it here for downstream processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Creating & processing the ``market_value`` feature\n",
    "There are two features in the ``extracted_df`` dataset that could conceivably be used as the target feature in our machine learning models to predict the market value of a given Jazz album, namely ``market_price`` and ``median``. Without missing data, it would be preferable to use the ``median`` price for the ``market_value`` of a given record, as the value of ``market_price`` is contingent on the period in which the data was scraped and may not be an accurate representation of what price the album is generally valued at in the market. Furthermore, as the ``market_price`` is the lowest price that a given record is being sold for, it could be that the reason for the price being so low is due to the poor quality of the record, a feature which is not captured in our data but is available in separate listings view of the record, which has been left out of the scope of this project in the interest of time, as it would have required the scraping the results of another 350k web requests to the Discogs.com servers. \n",
    "However, as we can see below, we do not have access to the ``median`` price for around 150,000 albums, and as such a sole reliance on this attribute would mean we would no longer be able to incorporate these albums in our analysis. In order to minimize the amount of data that must be discarded, we take the approach of combining the ``market_price`` and ``median`` features by filling the missing values of ``median`` with the values of ``market_price``. The rationale behind this combination of features is that a missing value for ``median_price`` indicates that the album in question has never been sold on the Discogs.com platform. Why this is the case could be down to a number of reasons, such as the album being fairly niche, new on the market or not particularly popular among the types of users that Discogs.com attracts. As such, we can noisily impute the value of the record by the supply-side valuation of the album in order to reduce the number of samples that must be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df.loc[:,['market_price','median']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_value_features = ColumnCombiner('median','market_price','market_value').fit_transform(extracted_df).loc[:,['market_value','median','market_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_value_features.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above, post-imputation it is possible to keep around 100,000 samples in the dataset for training our machine learning models. Nevertheless, we must still remove the entries for which ``market_value`` is null, as well as those samples that have outlier values, which we achieve using the ``NullRemover`` and ``OutlierRemover`` transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutlierRemover('market_value').fit_transform(\n",
    "    NullRemover('market_value').fit_transform(\n",
    "        market_value_features\n",
    "    )\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output above, by removing the nulls and outliers from the ``market_value`` feature, we manage to obtain a very similar distribution to the ``median`` feature we would have otherwise preferred from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del market_value_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the processing of ``api_data``, it is critical that we discard the duplicate values of ``release_id`` in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of duplicates is: ',len(extracted_df)-len(DuplicateRemover('release_id').fit_transform(extracted_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 - Count number of standards per Jazz album\n",
    "In an attempt to inject our data with some additional domain knowledge, we will analyze the ``track_titles`` feature of the ``extracted_data`` DataFrame and seek to identify the number of Jazz standards that each album has on its tracklist. Jazz Standards are pieces of music which are commonly covered by artists within the Jazz community, and provide a possible entrypoint of prospective customers to purchasing the record, as they may be more likely to recognize a certain track on a record, and therefore be more incentivized to buy it. Generally, audiences enjoy hearing another artist's take on an old classic, which makes this feature of Jazz music culture a possibly informative feature to be added for our modelling. \n",
    "We accomplish this via a similar method as was employed in the ``FeatureCleanReduce`` inheriting estimators, by using TF-IDF with n-grams in order to compare the ``track_titles`` for each album with a third-party list of accepted Jazz standards (https://en.wikipedia.org/wiki/List_of_jazz_standards), and attempting to find high probability matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standards = StandardCountEncoder(path=DATA_PATH).fit_transform(\n",
    "    Unpickler('track_titles').fit_transform(extracted_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del standards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Features\n",
    "#### Loading and Overview\n",
    "For the final component dataset that we will use to model the meta-characteristics of Jazz albums, we will utilize a learned representation of the cover images of each album and feed this as input into our final machine learning model. This step serves to act as the visual information the customer considers when evaluating a given album for purchase. \n",
    "\n",
    "In order to obtain the learned representation, we followed the following steps:\n",
    "1. Download the cover images for each release_id in the ``api_data`` dataset\n",
    "    a. This was done while scraping the data from Discogs.com for the ``extracted_data`` dataset, as the cover images are to be found on these pages\n",
    "    b. This step is enabled by the ImageScraper Class to be found in the data/util/scrape.py module\n",
    "2. Utilize a pre-trained Neural Network, MobileNetV2 in this case, from TFHub to extract the high level features from each of the cover images\n",
    "    a. For the pre-trained model used in this project, we obtain a vector of 1280 high-level features representing the learned representation of each image by the MobileNetV2 model\n",
    "    b. This step is enabled by the FeatureExtractor Class to be found in the data/util/scrape.py module\n",
    "    \n",
    "The scraping and feature extraction has been finalized in advance, and the high-level feature matrix has been saved in the ``data/data/high_level_features.npz`` file. We load the file below and display it for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(os.path.join(DATA_PATH,'high_level_features.npz')) as file:\n",
    "    high_level_features_df = pd.DataFrame(file['data'].reshape(350330,1280))\n",
    "    high_level_features_df['release_id'] = file['label']\n",
    "    high_level_features_df['release_id'] = high_level_features_df['release_id'].str.replace('bitmap_','').str.replace('.png','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_level_features_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
