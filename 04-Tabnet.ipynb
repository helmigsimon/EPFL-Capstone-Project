{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQQCreB5gAR_"
   },
   "outputs": [],
   "source": [
    "def upgrade_runtime_ram():\n",
    "    meminfo = subprocess.getoutput('cat /proc/meminfo').split('\\n')\n",
    "\n",
    "    memory_info = {entry.split(':')[0]: int(entry.split(':')[1].replace(' kB','').strip()) for entry in meminfo}\n",
    "\n",
    "    if memory_info['MemTotal'] > 17000000:\n",
    "        return\n",
    "\n",
    "    a = []\n",
    "    while(1):\n",
    "        a.append('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "waACZBi_8ap2"
   },
   "outputs": [],
   "source": [
    "def restart_runtime():\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YIsGqbsy8aqB"
   },
   "outputs": [],
   "source": [
    "def setup_rapids():\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    device_name = pynvml.nvmlDeviceGetName(handle)\n",
    "    if (device_name != b'Tesla T4') and (device_name != b'Tesla P4') and (device_name != b'Tesla P100-PCIE-16GB'):\n",
    "        print(\"Wrong GPU - Restarting Runtime\")\n",
    "        restart_runtime()\n",
    "\n",
    "\n",
    "    # clone RAPIDS AI rapidsai-csp-utils scripts repo\n",
    "    !git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "\n",
    "    # install RAPIDS\n",
    "    !bash rapidsai-csp-utils/colab/rapids-colab.sh 0.13\n",
    "\n",
    "\n",
    "    # set necessary environment variables \n",
    "    dist_package_index = sys.path.index('/usr/local/lib/python3.6/dist-packages')\n",
    "    sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.6/site-packages'] + sys.path[dist_package_index:]\n",
    "    sys.path\n",
    "\n",
    "    # update pyarrow & modules \n",
    "    exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9CcPvNIo8aqI"
   },
   "outputs": [],
   "source": [
    "def setup_conda():\n",
    "    if not 'Miniconda3-4.5.4-Linux-x86_64.sh' in os.listdir():\n",
    "        !wget https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh && bash Miniconda3-4.5.4-Linux-x86_64.sh -bfp /usr/local\n",
    "\n",
    "    if not ('EPFL-Capstone-Project' in os.listdir()) and (os.getcwd().split('/')[-1] != 'EPFL-Capstone-Project'):\n",
    "        !git clone https://github.com/helmigsimon/EPFL-Capstone-Project  \n",
    "    if 'EPFL-Capstone-Project' in os.listdir():\n",
    "        os.chdir('EPFL-Capstone-Project')\n",
    "\n",
    "    !conda env create -f environment.yml\n",
    "    !conda activate exts-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHSZ-lgE8aqP"
   },
   "outputs": [],
   "source": [
    "def setup_drive():\n",
    "    #Mounting Google Drive\n",
    "    global drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "N_uPJqb58aqW",
    "outputId": "a2c4cbdc-e3fa-4f89-b420-5d99d2e7f699"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import sys,os,subprocess\n",
    "    \n",
    "    upgrade_runtime_ram()\n",
    "    setup_drive()\n",
    "\n",
    "    #Setting up PyPi Packages\n",
    "    !pip install geopandas sparse-dot-topn pdpipe category-encoders\n",
    "    import geopandas as gpd\n",
    "    import sparse_dot_topn.sparse_dot_topn as ct\n",
    "    import pdpipe as pdp\n",
    "    import category_encoders\n",
    "\n",
    "    #Setting up Conda Packages\n",
    "    setup_conda()\n",
    "    !pip install tabnet[gpu]\n",
    "    \n",
    "    #Initializing NLTK\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "    #Setting up RAPIDS AI\n",
    "    import pynvml\n",
    "    setup_rapids()\n",
    "    \n",
    "    from cuml import UMAP\n",
    "    \n",
    "except ModuleNotFoundError as e:\n",
    "    print(e)\n",
    "    print('Not in colab environment, continuing to run locally')\n",
    "    from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tabnet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from category_encoders import LeaveOneOutEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.processing import load_from_pkl\n",
    "from data.util.paths import DATA_PATH\n",
    "from lib.pipelines import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df, extracted_df = load_from_pkl('api',DATA_PATH), load_from_pkl('extracted',DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df = api_pipe.fit_transform(api_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df = extracted_pipe.fit_transform(extracted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(os.path.join(DATA_PATH,'high_level_features_labelled.npz')) as data:\n",
    "    image_embedding_df = pd.concat([pd.DataFrame(data[section]) for section in ('release_id','bitmap','features')],axis=1)\n",
    "    image_embedding_df.columns = ['release_id', 'bitmap'] + ['feature_%s' % i for i in range(1,1281)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46fQY6W28arC"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=10)\n",
    "image_embeddings_scaled = scaler.fit_transform(image_embedding_df.loc[:,['feature_%s' % i for i in range(1,1281)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "-DiegN158q_u",
    "outputId": "3be17565-924f-45d0-ccf6-15d1e8afa25c"
   },
   "outputs": [],
   "source": [
    "image_embeddings_reduced = pca.fit_transform(image_embeddings_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9O2_hoRwzSio"
   },
   "outputs": [],
   "source": [
    "image_embeddings_reduced = pd.concat([\n",
    "      image_embedding_df.loc[:,'release_id'],\n",
    "      pd.DataFrame(\n",
    "          image_embeddings_reduced,\n",
    "          columns = ['images_umap_%s' % i for i in range(image_embeddings_reduced.shape[1])]\n",
    "      )],\n",
    "      axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZgR3OYwl_Yy"
   },
   "source": [
    "Combining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSZb0b4JT-72"
   },
   "outputs": [],
   "source": [
    "df = api_df.merge(extracted_df,how='inner',on='release_id')\n",
    "df = df.merge(image_embeddings_reduced,how='inner',on='release_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DBmeMFEUJ0N"
   },
   "outputs": [],
   "source": [
    "del api_df, extracted_df, image_embedding_df, image_embeddings_scaled, image_embeddings_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCI8WWUu8ard"
   },
   "outputs": [],
   "source": [
    "record_store_tabnet_transformer = ColumnTransformer(transformers=[\n",
    "    ('year_encoder', OneHotEncoder(dtype=np.uint8,handle_unknown='ignore'), ['year']),\n",
    "    ('format_name', OneHotEncoder(dtype=np.uint8,handle_unknown='ignore'), ['format_name'])\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZHfrogk8arh"
   },
   "outputs": [],
   "source": [
    "record_store_tabnet_removal_columns = [\n",
    "    'market_price','units_for_sale','have','want','average_rating','rating_count','last_sold','lowest','median',\n",
    "    'highest','track_titles','country','genre','style','community_have','community_want','formats','thumb_url',\n",
    "    'release_url','format_description','days_since_last_sale','title',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRH_P30fEM-W"
   },
   "outputs": [],
   "source": [
    "record_store_tabnet_pipe = Pipeline([\n",
    "    ('running_time_imputer',RunningTimeImputer('running_time','number_of_tracks')),\n",
    "    ('leave_one_out_encoding', LeaveOneOutEncoder(cols=['artist','label','format_text','master_id'])),\n",
    "    ('record_store_column_remover', ColumnRemover(record_store_tabnet_removal_columns)),\n",
    "    ('preprocessing',record_store_tabnet_transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arr = record_store_tabnet_pipe.fit_transform(df.drop('market_value',axis=1),df.market_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pretensor = np.concatenate([df_arr,np.log(df['market_value']).values.reshape(-1,1)],axis=1)\n",
    "np.random.shuffle(df_pretensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(df_pretensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(ds):\n",
    "    unstacked = tf.unstack(ds)\n",
    "    features = unstacked[:-1]\n",
    "    \n",
    "    x = dict(zip(col_names,features))\n",
    "    y = unstacked[-1]\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['column_%s' % i for i in range(975)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8*len(df))\n",
    "BATCH_SIZE = 1000\n",
    "ds_train = ds.take(train_size)\n",
    "ds_train = ds_train.map(transform)\n",
    "ds_train = ds_train.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds.skip(train_size)\n",
    "ds_test = ds_test.map(transform)\n",
    "ds_test = ds_test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(column) for column in col_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tabnet.TabNetRegressor(\n",
    "    feature_columns,\n",
    "    feature_dim=64,\n",
    "    output_dim=64,\n",
    "    num_regressors=len(feature_columns),\n",
    "    num_decision_steps=2,\n",
    "    relaxation_factor=1,\n",
    "    sparsity_coefficient=1e-2,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01,decay_steps=100,decay_rate=0.9,staircase=False)\n",
    "optimizer = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer,loss='mean_squared_error',metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(ds_train,epochs=50,validation_data=ds_test,verbose=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
