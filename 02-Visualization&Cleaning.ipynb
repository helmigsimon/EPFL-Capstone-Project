{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "02-Visualization&Cleaning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4M6H89QGLNE",
        "colab_type": "text"
      },
      "source": [
        "## 02 - Visualization & Cleaning\n",
        "In this notebook, we will visualize and further interrogate the datasets processed in the 01-Processing notebook in order to draw some more detailed insights regarding the characteristics of the data as it relates to our goal of predicting the market value of Jazz albums. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YsMW-elZopc_"
      },
      "source": [
        "## Setting up Colab Environment if in Colab\n",
        "A key component of the interrogation of our data will involve the dimensionality reduction of the high-level features of the image data we have obtained using the pre-trained MobileNetV2 Model introduced in the previous notebook. Due to the high dimensionality and cardinality of our data, and our choice of the UMAP algorithm as the dimensionality reduction tool of choice in this project, we will make use of the RAPIDS AI CUML library (https://docs.rapids.ai/api), which provides a sci-kit learn-like API for implementations of machine learning algorithms that are specifically configured to run on GPU hardware. Using this library will significantly speed up the computation time associated with the dimensionality reduction we conduct towards the end of this notebook.\n",
        "\n",
        "As I do not personally have access to a GPU, the GPU-enabled part of this notebook is run on Google's Colab Notebook environment, which offers GPU access for free. In the cell belows below, we define the functions required to setup a 25GB RAM Colab Notebook Environment with the packages necessary for the code to function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qQQCreB5gAR_",
        "colab": {}
      },
      "source": [
        "def upgrade_runtime_ram():\n",
        "    \"\"\"Crash the Colab Runtime if in a low RAM environment. This prompts an upgrade to a high RAM runtime.\n",
        "    \"\"\"\n",
        "    meminfo = subprocess.getoutput('cat /proc/meminfo').split('\\n')\n",
        "\n",
        "    memory_info = {entry.split(':')[0]: int(entry.split(':')[1].replace(' kB','').strip()) for entry in meminfo}\n",
        "\n",
        "    if memory_info['MemTotal'] > 17000000:\n",
        "        return\n",
        "\n",
        "    a = []\n",
        "    while(1):\n",
        "        a.append('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "waACZBi_8ap2",
        "colab": {}
      },
      "source": [
        "def restart_runtime():\n",
        "    \"\"\"Restart the Colab Runtime\n",
        "    \"\"\"\n",
        "    os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YIsGqbsy8aqB",
        "colab": {}
      },
      "source": [
        "def setup_rapids():\n",
        "    \"\"\"Sets up RAPIDS for Colab by testing the Runtime for compatibility and downloading and update RAPIDS packages and dependencies\n",
        "    \"\"\"\n",
        "    def test_runtime_compatibility():\n",
        "        \"\"\"Test whether RAPIDS can be run on the given Colab Runtime. Restarts the Runtime if RAPIDS can not be run.\n",
        "        \"\"\"\n",
        "        import pynvml\n",
        "        pynvml.nvmlInit()\n",
        "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "        device_name = pynvml.nvmlDeviceGetName(handle)\n",
        "        if (device_name != b'Tesla T4') and (device_name != b'Tesla P4') and (device_name != b'Tesla P100-PCIE-16GB'):\n",
        "            print(\"Wrong GPU - Restarting Runtime\")\n",
        "            restart_runtime()\n",
        "            \n",
        "    def download_and_update_rapids():\n",
        "        # clone RAPIDS AI rapidsai-csp-utils scripts repo\n",
        "        !git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "\n",
        "        # install RAPIDS\n",
        "        !bash rapidsai-csp-utils/colab/rapids-colab.sh 0.13\n",
        "\n",
        "        # set necessary environment variables \n",
        "        dist_package_index = sys.path.index('/usr/local/lib/python3.6/dist-packages')\n",
        "        sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.6/site-packages'] + sys.path[dist_package_index:]\n",
        "        sys.path\n",
        "\n",
        "        # update pyarrow & modules \n",
        "        exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())\n",
        "        \n",
        "    test_runtime_compatibility()\n",
        "    download_and_update_rapids()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9CcPvNIo8aqI",
        "colab": {}
      },
      "source": [
        "def setup_conda():\n",
        "    if not 'Miniconda3-4.5.4-Linux-x86_64.sh' in os.listdir():\n",
        "        !wget https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh && bash Miniconda3-4.5.4-Linux-x86_64.sh -bfp /usr/local\n",
        "\n",
        "    if not ('EPFL-Capstone-Project' in os.listdir()) and (os.getcwd().split('/')[-1] != 'EPFL-Capstone-Project'):\n",
        "        !git clone https://github.com/helmigsimon/EPFL-Capstone-Project  \n",
        "    if 'EPFL-Capstone-Project' in os.listdir():\n",
        "        os.chdir('EPFL-Capstone-Project')\n",
        "\n",
        "    !conda env create -f environment.yml\n",
        "    !conda activate exts-ml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WHSZ-lgE8aqP",
        "colab": {}
      },
      "source": [
        "def setup_drive():\n",
        "    #Mounting Google Drive\n",
        "    global drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N_uPJqb58aqW",
        "outputId": "ef6cf498-d7ad-40fe-fcb9-8d6c6675db6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "try:\n",
        "    import sys,os,subprocess\n",
        "    \n",
        "    upgrade_runtime_ram()\n",
        "    setup_drive()\n",
        "\n",
        "    #Setting up PyPi Packages\n",
        "    !pip install geopandas sparse-dot-topn pdpipe category-encoders catboost\n",
        "    import geopandas as gpd\n",
        "    import sparse_dot_topn.sparse_dot_topn as ct\n",
        "    import pdpipe as pdp\n",
        "    import category_encoders\n",
        "\n",
        "    #Setting up Conda Packages\n",
        "    setup_conda()\n",
        "    \n",
        "    #Initializing NLTK\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt')\n",
        "    \n",
        "    #Setting up RAPIDS AI\n",
        "    #setup_rapids()\n",
        "    import pynvml\n",
        "    pynvml.nvmlInit()\n",
        "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "    device_name = pynvml.nvmlDeviceGetName(handle)\n",
        "    if (device_name != b'Tesla T4') and (device_name != b'Tesla P4') and (device_name != b'Tesla P100-PCIE-16GB'):\n",
        "        print(\"Wrong GPU - Restarting Runtime\")\n",
        "        restart_runtime()\n",
        "\n",
        "    # clone RAPIDS AI rapidsai-csp-utils scripts repo\n",
        "    !git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "\n",
        "    # install RAPIDS\n",
        "    !bash rapidsai-csp-utils/colab/rapids-colab.sh 0.13\n",
        "\n",
        "    # set necessary environment variables \n",
        "    dist_package_index = sys.path.index('/usr/local/lib/python3.6/dist-packages')\n",
        "    sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.6/site-packages'] + sys.path[dist_package_index:]\n",
        "    sys.path\n",
        "\n",
        "    # update pyarrow & modules \n",
        "    exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())\n",
        "\n",
        "    from cuml import UMAP\n",
        "    \n",
        "except ModuleNotFoundError as e:\n",
        "    print(e)\n",
        "    print('Not in colab environment, continuing to run locally')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Collecting geopandas\n",
            "  Using cached https://files.pythonhosted.org/packages/83/c5/3cf9cdc39a6f2552922f79915f36b45a95b71fd343cfc51170a5b6ddb6e8/geopandas-0.7.0-py2.py3-none-any.whl\n",
            "Collecting sparse-dot-topn\n",
            "Collecting pdpipe\n",
            "  Using cached https://files.pythonhosted.org/packages/44/dc/6f4bd0b02517d19f94e10c1fbed9177bf173b2aceb63cf930ff4df9348fc/pdpipe-0.0.49-py3-none-any.whl\n",
            "Collecting category-encoders\n",
            "  Using cached https://files.pythonhosted.org/packages/44/57/fcef41c248701ee62e8325026b90c432adea35555cbc870aff9cfba23727/category_encoders-2.2.2-py2.py3-none-any.whl\n",
            "Collecting catboost\n",
            "  Using cached https://files.pythonhosted.org/packages/b2/aa/e61819d04ef2bbee778bf4b3a748db1f3ad23512377e43ecfdc3211437a0/catboost-0.23.2-cp36-none-manylinux1_x86_64.whl\n",
            "Collecting pyproj>=2.2.0 (from geopandas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/40/772067a7532f3dc745caf5c0fddba667fc7eef3eccbd7963d34fa59feaa6/pyproj-2.6.1.post1-cp36-cp36m-manylinux1_x86_64.whl (10.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 10.9MB 5.1MB/s \n",
            "\u001b[?25hCollecting pandas>=0.23.0 (from geopandas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/71/8f53bdbcbc67c912b888b40def255767e475402e9df64050019149b1a943/pandas-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (10.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 10.0MB 5.5MB/s \n",
            "\u001b[?25hCollecting shapely (from geopandas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/fa/c96d3461fda99ed8e82ff0b219ac2c8384694b4e640a611a1a8390ecd415/Shapely-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.8MB 1.8MB/s \n",
            "\u001b[?25hCollecting fiona (from geopandas)\n",
            "  Using cached https://files.pythonhosted.org/packages/ec/20/4e63bc5c6e62df889297b382c3ccd4a7a488b00946aaaf81a118158c6f09/Fiona-1.8.13.post1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/site-packages (from sparse-dot-topn) (39.2.0)\n",
            "Collecting numpy>=1.16.6 (from sparse-dot-topn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/27/e35e7c6e6a52fab9fcc64fc2b20c6b516eba930bb02b10ace3b38200d3ab/numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl (20.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 20.2MB 2.7MB/s \n",
            "\u001b[?25hCollecting cython>=0.29.15 (from sparse-dot-topn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/37/13249e34be215c30b4dbfcd890c5ab719a01b252288260e0cf3035ce130f/Cython-0.29.19-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 14.8MB/s \n",
            "\u001b[?25hCollecting scipy>=1.2.3 (from sparse-dot-topn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 26.1MB 2.2MB/s \n",
            "\u001b[?25hCollecting strct (from pdpipe)\n",
            "  Using cached https://files.pythonhosted.org/packages/4a/c4/bd0de8562cb3731bdfcc33a4627c755a2d9c5e6e96b73546134c43acf1be/strct-0.0.30-py2.py3-none-any.whl\n",
            "Collecting sortedcontainers (from pdpipe)\n",
            "  Downloading https://files.pythonhosted.org/packages/13/f3/cf85f7c3a2dbd1a515d51e1f1676d971abe41bba6f4ab5443240d9a78e5b/sortedcontainers-2.1.0-py2.py3-none-any.whl\n",
            "Collecting tqdm (from pdpipe)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/40/058b12e8ba10e35f89c9b1fdfc2d4c7f8c05947df2d5eb3c7b258019fda0/tqdm-4.46.0-py2.py3-none-any.whl (63kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 30.1MB/s \n",
            "\u001b[?25hCollecting skutil>=0.0.15 (from pdpipe)\n",
            "  Using cached https://files.pythonhosted.org/packages/34/2b/1b5c9e7be3c24e1bd5ce35c2d27a5780989c3d90fcee10f3fee3074dda7f/skutil-0.0.16-py2.py3-none-any.whl\n",
            "Collecting statsmodels>=0.9.0 (from category-encoders)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/83/540fd83238a18abe6c2d280fa8e489ac5fcefa1f370f0ca1acd16ae1b860/statsmodels-0.11.1-cp36-cp36m-manylinux1_x86_64.whl (8.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 8.7MB 2.5MB/s \n",
            "\u001b[?25hCollecting patsy>=0.5.1 (from category-encoders)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/0c/5f61f1a3d4385d6bf83b83ea495068857ff8dfb89e74824c6e9eb63286d8/patsy-0.5.1-py2.py3-none-any.whl (231kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 24.1MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.20.0 (from category-encoders)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/3a/eb8d7bbe28f4787d140bb9df685b7d5bf6115c0e2a969def4027144e98b6/scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 6.9MB 1.2MB/s \n",
            "\u001b[?25hCollecting graphviz (from catboost)\n",
            "  Downloading https://files.pythonhosted.org/packages/83/cc/c62100906d30f95d46451c15eb407da7db201e30f42008f3643945910373/graphviz-0.14-py2.py3-none-any.whl\n",
            "Collecting plotly (from catboost)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/56/eabdc7b7187cdb9d6121f6de2831ad5b85f7d002fa4bfe0476dbdb554bf6/plotly-4.8.1-py2.py3-none-any.whl (11.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 11.5MB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from catboost) (1.11.0)\n",
            "Collecting matplotlib (from catboost)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/4b/52da6b1523d5139d04e02d9e26ceda6146b48f2a4e5d2abfdf1c7bac8c40/matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.4MB 4.7MB/s \n",
            "\u001b[?25hCollecting pytz>=2017.2 (from pandas>=0.23.0->geopandas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl (510kB)\n",
            "\u001b[K    100% |████████████████████████████████| 512kB 15.7MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.6.1 (from pandas>=0.23.0->geopandas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 15.5MB/s \n",
            "\u001b[?25hCollecting cligj>=0.5 (from fiona->geopandas)\n",
            "  Using cached https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl\n",
            "Collecting click<8,>=4.0 (from fiona->geopandas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 27.9MB/s \n",
            "\u001b[?25hCollecting munch (from fiona->geopandas)\n",
            "  Using cached https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Collecting click-plugins>=1.0 (from fiona->geopandas)\n",
            "  Using cached https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n",
            "Collecting attrs>=17 (from fiona->geopandas)\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/db/4313ab3be961f7a763066401fb77f7748373b6094076ae2bda2806988af6/attrs-19.3.0-py2.py3-none-any.whl\n",
            "Collecting decore (from skutil>=0.0.15->pdpipe)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.20.0->category-encoders)\n",
            "  Downloading https://files.pythonhosted.org/packages/db/09/cab2f398e28e9f183714afde872b2ce23629f5833e467b151f18e1e08908/threadpoolctl-2.0.0-py3-none-any.whl\n",
            "Collecting joblib>=0.11 (from scikit-learn>=0.20.0->category-encoders)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/a6/d1a816b89aa1e9e96bcb298eb1ee1854f21662ebc6d55ffa3d7b3b50122b/joblib-0.15.1-py3-none-any.whl (298kB)\n",
            "\u001b[K    100% |████████████████████████████████| 307kB 22.7MB/s \n",
            "\u001b[?25hCollecting retrying>=1.3.3 (from plotly->catboost)\n",
            "  Downloading https://files.pythonhosted.org/packages/44/ef/beae4b4ef80902f22e3af073397f079c96969c69b2c7d52a57ea9ae61c9d/retrying-1.3.3.tar.gz\n",
            "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->catboost)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 27.2MB/s \n",
            "\u001b[?25hCollecting cycler>=0.10 (from matplotlib->catboost)\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib->catboost)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/23/147de658aabbf968324551ea22c0c13a00284c4ef49a77002e91f79657b7/kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 27.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: retrying\n",
            "  Running setup.py bdist_wheel for retrying ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d7/a9/33/acc7b709e2a35caa7d4cae442f6fe6fbf2c43f80823d46460c\n",
            "Successfully built retrying\n",
            "Installing collected packages: pyproj, pytz, python-dateutil, numpy, pandas, shapely, click, cligj, munch, click-plugins, attrs, fiona, geopandas, cython, scipy, sparse-dot-topn, strct, sortedcontainers, tqdm, decore, skutil, pdpipe, patsy, statsmodels, threadpoolctl, joblib, scikit-learn, category-encoders, graphviz, retrying, plotly, pyparsing, cycler, kiwisolver, matplotlib, catboost\n",
            "Successfully installed attrs-19.3.0 catboost-0.23.2 category-encoders-2.2.2 click-7.1.2 click-plugins-1.1.1 cligj-0.5.0 cycler-0.10.0 cython-0.29.19 decore-0.0.1 fiona-1.8.13.post1 geopandas-0.7.0 graphviz-0.14 joblib-0.15.1 kiwisolver-1.2.0 matplotlib-3.2.1 munch-2.5.0 numpy-1.18.4 pandas-1.0.3 patsy-0.5.1 pdpipe-0.0.49 plotly-4.8.1 pyparsing-2.4.7 pyproj-2.6.1.post1 python-dateutil-2.8.1 pytz-2020.1 retrying-1.3.3 scikit-learn-0.23.1 scipy-1.4.1 shapely-1.7.0 skutil-0.0.16 sortedcontainers-2.1.0 sparse-dot-topn-0.2.9 statsmodels-0.11.1 strct-0.0.30 threadpoolctl-2.0.0 tqdm-4.46.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "attr",
                  "cycler",
                  "dateutil",
                  "fiona",
                  "geopandas",
                  "joblib",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pandas",
                  "pyparsing",
                  "pyproj",
                  "scipy",
                  "shapely"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bfailed\n",
            "\n",
            "ResolvePackageNotFound: \n",
            "  - openjpeg==2.3.1=h254dc36_3\n",
            "  - psycopg2==2.8.4=py36hafa8578_1\n",
            "  - pandas==1.0.1=py36h4f17bb1_0\n",
            "  - xz==5.2.4=h0b31af3_1002\n",
            "  - glib==2.58.3=py36hb0ce7ff_1003\n",
            "  - scikit-learn==0.20.3=py36hca23c92_1\n",
            "  - c-ares==1.15.0=h01d97ff_1001\n",
            "  - hdf5==1.10.5=nompi_h3e39495_1104\n",
            "  - libnetcdf==4.7.4=nompi_he461dc0_101\n",
            "  - statsmodels==0.11.1=py36h37b9a7d_1\n",
            "  - libxml2==2.9.10=h53d96d6_0\n",
            "  - pcre==8.44=h4a8c4bd_0\n",
            "  - libopenblas==0.3.9=h3d69b6c_0\n",
            "  - libpq==12.2=h554dc5a_0\n",
            "  - pyzmq==19.0.0=py36h820b253_1\n",
            "  - libpng==1.6.37=hbbe82c9_1\n",
            "  - kealib==1.4.12=h2324030_0\n",
            "  - freexl==1.0.5=h1de35cc_1002\n",
            "  - graphviz==2.42.3=h98dfb87_0\n",
            "  - libspatialite==4.3.0a=hbcd37d4_1037\n",
            "  - libtiff==4.1.0=ha78913b_3\n",
            "  - gettext==0.19.8.1=h46ab8bc_1002\n",
            "  - cryptography==2.8=py36hc9d8292_2\n",
            "  - json-c==0.13.1=h1de35cc_1001\n",
            "  - h5py==2.10.0=nompi_py36h106b333_102\n",
            "  - geotiff==1.5.1=h4bdff65_9\n",
            "  - gdal==3.0.4=py36h5a1ef21_3\n",
            "  - libiconv==1.15=h0b31af3_1006\n",
            "  - lz4-c==1.8.3=h6de7cb9_1001\n",
            "  - pixman==0.38.0=h01d97ff_1003\n",
            "  - pillow==7.0.0=py36h918e99a_0\n",
            "  - bzip2==1.0.8=h0b31af3_2\n",
            "  - sparse_dot_topn==0.2.9=py36h863e41a_0\n",
            "  - llvmlite==0.31.0=py36hde82470_1\n",
            "  - shapely==1.7.0=py36hda0b814_2\n",
            "  - libgdal==3.0.4=hec54991_3\n",
            "  - tensorflow==1.12.0=mkl_py36h2b2bbaf_0\n",
            "  - fiona==1.8.13=py36he71f6a4_0\n",
            "  - icu==64.2=h6de7cb9_1\n",
            "  - libprotobuf==3.11.4=hd174df1_0\n",
            "  - giflib==5.2.1=h0b31af3_2\n",
            "  - hdf4==4.2.13=h84186c3_1003\n",
            "  - python==3.6.10=hce46be0_1009_cpython\n",
            "  - zeromq==4.3.2=h6de7cb9_2\n",
            "  - jpeg==9c=h1de35cc_1001\n",
            "  - libffi==3.2.1=h4a8c4bd_1007\n",
            "  - scipy==1.4.1=py36h82752d6_0\n",
            "  - curl==7.68.0=h8754def_0\n",
            "  - xerces-c==3.2.2=h8f8adb3_1004\n",
            "  - cffi==1.14.0=py36h356ff06_0\n",
            "  - numpy==1.18.1=py36hde6bac1_0\n",
            "  - matplotlib-base==3.2.0=py36h11da6c2_1\n",
            "  - libsodium==1.0.17=h01d97ff_0\n",
            "  - tornado==6.0.4=py36h37b9a7d_1\n",
            "  - cairo==1.16.0=hec6a9b0_1003\n",
            "  - nodejs==13.10.1=h38d8c5a_0\n",
            "  - grpcio==1.23.0=py36h7c1f37e_1\n",
            "  - mistune==0.8.4=py36h0b31af3_1000\n",
            "  - kiwisolver==1.1.0=py36h863e41a_1\n",
            "  - libdap4==3.20.4=habf5908_0\n",
            "  - libedit==3.1.20170329=hcfe32e1_1001\n",
            "  - libcurl==7.68.0=h709d2b2_0\n",
            "  - libllvm8==8.0.1=h770b8ee_0\n",
            "  - openssl==1.1.1f=h0b31af3_0\n",
            "  - fontconfig==2.13.1=h6b1039f_1001\n",
            "  - tensorflow-base==1.12.0=mkl_py36h70e0e9a_0\n",
            "  - python-levenshtein==0.12.0=py36h0b31af3_1001\n",
            "  - zlib==1.2.11=h0b31af3_1006\n",
            "  - numba==0.48.0=py36h4f17bb1_0\n",
            "  - tiledb==1.7.0=hd5e958f_2\n",
            "  - tzcode==2019a=h01d97ff_1002\n",
            "  - readline==8.0=hcfe32e1_0\n",
            "  - sqlite==3.30.1=h93121df_0\n",
            "  - libkml==1.3.0=h169b8f9_1011\n",
            "  - expat==2.2.9=h4a8c4bd_2\n",
            "  - markupsafe==1.1.1=py36h37b9a7d_1\n",
            "  - postgresql==12.2=h16d8c28_0\n",
            "  - psutil==5.7.0=py36h37b9a7d_1\n",
            "  - libssh2==1.8.2=hcdc9a53_2\n",
            "  - libuv==1.34.0=h0b31af3_0\n",
            "  - poppler==0.67.0=h16886b5_8\n",
            "  - proj==6.3.1=h773a61f_1\n",
            "  - geos==3.8.1=h4a8c4bd_0\n",
            "  - llvm-openmp==9.0.1=h28b9765_2\n",
            "  - libwebp==1.0.2=hd3bf737_5\n",
            "  - pyrsistent==0.16.0=py36h37b9a7d_0\n",
            "  - krb5==1.16.4=h1752a42_0\n",
            "  - ncurses==6.1=h0a44026_1002\n",
            "  - cfitsio==3.470=h84d2f63_2\n",
            "  - sqlalchemy==1.3.13=py36h0b31af3_0\n",
            "  - freetype==2.10.1=h8da9a1a_0\n",
            "  - boost-cpp==1.72.0=hdf9ef73_0\n",
            "  - zstd==1.4.4=hed8d7c8_2\n",
            "  - libgfortran==4.0.0=2\n",
            "  - appnope==0.1.0=py36h9f0ad1d_1001\n",
            "  - tk==8.6.10=hbbe82c9_0\n",
            "\n",
            "\n",
            "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n",
            "If your shell is Bash or a Bourne variant, enable conda for the current user with\n",
            "\n",
            "    $ echo \". /usr/local/etc/profile.d/conda.sh\" >> ~/.bashrc\n",
            "\n",
            "or, for all users, enable conda with\n",
            "\n",
            "    $ sudo ln -s /usr/local/etc/profile.d/conda.sh /etc/profile.d/conda.sh\n",
            "\n",
            "The options above will permanently enable the 'conda' command, but they do NOT\n",
            "put conda's base (root) environment on PATH.  To do so, run\n",
            "\n",
            "    $ conda activate\n",
            "\n",
            "in your terminal, or to put the base environment on PATH permanently, run\n",
            "\n",
            "    $ echo \"conda activate\" >> ~/.bashrc\n",
            "\n",
            "Previous to conda 4.4, the recommended way to activate conda was to modify PATH in\n",
            "your ~/.bashrc file.  You should manually remove the line that looks like\n",
            "\n",
            "    export PATH=\"/usr/local/bin:$PATH\"\n",
            "\n",
            "^^^ The above line should NO LONGER be in your ~/.bashrc file! ^^^\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9jIRARPsopdO",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import umap\n",
        "from tqdm import tqdm\n",
        "from data.util.paths import DATA_PATH\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from lib.transformers import *\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "tqdm.pandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kxi6uxZfopda",
        "colab": {}
      },
      "source": [
        "from lib.processing import *\n",
        "from lib.pipelines import *\n",
        "from data.scripts.project_data import DataLoader\n",
        "from data.util.environment_variables import COUNTRY_CODES, M49_TO_ISO3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7e19AUp0opdo"
      },
      "source": [
        "## Setting up Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XP_r-y2GLOC",
        "colab_type": "text"
      },
      "source": [
        "As a first step, we load the datasets that original ``extracted_data`` and ``api_data`` datasets, apply the pipelines outlined in the previous notebook, and merge them to create our core dataset, referred to henceforth as ``df``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mzqe0h3Topdr",
        "outputId": "299242d5-740b-421d-e796-922da9cc1e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "extracted_df, api_df = load_from_pkl('extracted',path=DATA_PATH), load_from_pkl('api',path=DATA_PATH)\n",
        "extracted_df = extracted_pipe.fit_transform(extracted_df)\n",
        "api_df = api_pipe.fit_transform(api_df)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d467e80ed7b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextracted_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_pkl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extracted'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_from_pkl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'api'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mextracted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextracted_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mapi_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_from_pkl' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GGrpPtwSopdy",
        "colab": {}
      },
      "source": [
        "df = pd.merge(extracted_df,api_df,on='release_id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xca-sd7Kopd6",
        "colab": {}
      },
      "source": [
        "del extracted_df, api_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-CSdSscKopd_"
      },
      "source": [
        "## Create Column Store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvkNLlqcGLOW",
        "colab_type": "text"
      },
      "source": [
        "In order to keep tabs on all of the subgroups of features that we are using in this project, we make use of the ``ColumnStore`` object, which uses the blueprint provided by the ``col_set`` variable to create a directory of column sets associated with our core dataset, ``df``. The ``ColumnStore`` object accepts strings which it matches with the column names it finds, collections of strings, which it saves as the columns associated with a given group, or functions, which it applies to the dataframe on the column axis to identify whether a given column belongs to the grouping or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7zRYsk69opeB",
        "colab": {}
      },
      "source": [
        "col_set = {\n",
        "    'format': {\n",
        "        'description': 'format_description_', \n",
        "        'name': 'format_name_', \n",
        "        'text': ('format_text_clean'),\n",
        "        'quantity': ('format_quantity')\n",
        "    },\n",
        "    'geography': {\n",
        "        'superregion': 'superregion_',\n",
        "        'region': 'region_',\n",
        "        'country': 'country_'\n",
        "    },\n",
        "    'timeperiod': {\n",
        "        'period': 'period_',\n",
        "        'era': 'era_'\n",
        "    },\n",
        "    'genre': 'genre_',\n",
        "    'style': 'style_',\n",
        "    'null': None,\n",
        "    'indicator': lambda x: x.max() == 1 and x.min() == 0,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zZGhQ6iEopeH",
        "colab": {}
      },
      "source": [
        "column_store = ColumnStore()\n",
        "column_store.fit(df,col_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSF-X29cGLOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "consolidation_pipe = make_data_consolidation_pipe(df,column_store)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m71v6OV_GLOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "consolidation_pipe.fit_transform(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jxouB3QGopeM"
      },
      "source": [
        "## Overview of Number of Albums over Time\n",
        "As an initial step to gain an overview of the development of the release of Jazz albums over the time period we are dealing with, we will plot this in a line plot, and superimpose the eras and periods associated with the time periods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KwVeuDqIopeO",
        "colab": {}
      },
      "source": [
        "period_year_range, era_year_range = make_year_range_dict(column_store._timeperiod_period,df=df), make_year_range_dict(column_store._timeperiod_era,df=df)\n",
        "time_period_year_range = dict(**period_year_range,**era_year_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CvLR6x8KopeV",
        "colab": {}
      },
      "source": [
        "year_count_series = df.groupby(by='year')['market_value'].count()\n",
        "year_count_series[1936] = 0\n",
        "year_count_series.sort_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6_daehdNopeb",
        "colab": {}
      },
      "source": [
        "period_colors = {\n",
        "    'period_big_band': 'red',\n",
        "    'period_bebop': 'pink',\n",
        "    'period_cool': 'blue',\n",
        "    'period_fusion': 'orange',\n",
        "}\n",
        "era_colors = {\n",
        "    'era_swing': 'purple',\n",
        "    'era_modern': 'green',\n",
        "    'era_contemporary': 'gold'\n",
        "}\n",
        "time_period_colors = dict(**period_colors,**era_colors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PiaymLFuopeh",
        "colab": {}
      },
      "source": [
        "def plot_variable_with_time_periods(year_series,time_periods,time_period_colors,**kwargs):\n",
        "    plt.figure(figsize=(20,10))\n",
        "\n",
        "    #Setting up plotting helpers\n",
        "    dev_constant = 200\n",
        "    convert_to_label = lambda x: ' '.join(x.split('_')).title()\n",
        "\n",
        "    #Plotting\n",
        "    plt.plot(year_series)\n",
        "    for time_period, year_range in time_periods.items():\n",
        "        if 'period' in time_period:\n",
        "            try:\n",
        "                min_, max_, hatch_ = 0, year_series.loc[year_range], '/'\n",
        "            except KeyError:\n",
        "                print(year_range)\n",
        "        else:\n",
        "            min_, max_, hatch_ = year_series.loc[year_range], year_series.loc[year_range]+dev_constant, None\n",
        "\n",
        "        plt.fill_between(year_range,max_,min_,alpha=0.25,color=time_period_colors[time_period],hatch=hatch_,label=convert_to_label(time_period))\n",
        "    \n",
        "    for attr in (kwargs):\n",
        "        try:\n",
        "            getattr(plt,attr)(kwargs[attr])\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_qboo8fopen",
        "colab": {}
      },
      "source": [
        "plot_variable_with_time_periods(year_count_series,\n",
        "                                time_period_year_range,\n",
        "                                time_period_colors,\n",
        "                                xlabel='Year',\n",
        "                                ylabel='Albums',\n",
        "                                title='Albums released per Year according to Dominant Jazz Time Period')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WcMiyqzcopet"
      },
      "source": [
        "As we can see from the above, there was a massive explosion in the number of Jazz albums released per year in the 1950s, with a cyclical rise through the rest of the 20th century and into the 21st. In the Contemporary Jazz Era/Jazz Fusion Period, we see that the number of Jazz albums released is at its highest. This could be due to the growing influence of Jazz on other music genres, and the heightened cross-pollination that has characterized music in the digital age. \n",
        "To further investigate this, we will limit the development of albums per year to those which are solely listed as Jazz albums, and do not have any other genres associated with them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gvrKI3mZopet",
        "colab": {}
      },
      "source": [
        "pure_jazz_album_count = df[df[column_store._genre].sum(axis=1)==0].groupby(by='year').count()['market_value']\n",
        "pure_jazz_album_count[1934], pure_jazz_album_count[1936] = 0,0\n",
        "pure_jazz_album_count.sort_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "doMiQs_yopey",
        "colab": {}
      },
      "source": [
        "plot_variable_with_time_periods(pure_jazz_album_count,\n",
        "                                time_period_year_range,\n",
        "                                time_period_colors,\n",
        "                                xlabel='Year',\n",
        "                                ylabel='Albums',\n",
        "                                title='Albums released per Year according to Dominant Jazz Time Period')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8goNt9H2ope3"
      },
      "source": [
        "As we can see, the trend of the figure above is effectively identical to the one which includes non 'pure' Jazz albums, indicating that the rising trend in album releases is not primarily due to the incorporation of Jazz into other styles, but instead a growth of the music in its \"purest\" form over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_5niykFLope4"
      },
      "source": [
        "## Market Value\n",
        "Having established the development of our object of interest over the time period considered in this project, let us now turn our attention to the feature we are trying to predict, namely ``market_value``, and identify it's specificities. To do so, we will consider the distribution of the feature, it's evolution over time and it's correlations with the rest of our features\n",
        "### Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LIm-OkBkope5",
        "colab": {}
      },
      "source": [
        "sns.distplot(df['market_value'])\n",
        "plt.ylabel('Distribution')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0mSsQKKCopfA"
      },
      "source": [
        "From the above we see that there is a substantial leftward skew for ``market_value``. Despite having initially removed outliers from the dataset we are considering using the ``OutlierRemover`` transformer in the ``extracted_pipe`` pipeline, we see that outliers continue to bias the distribution of our target variable.\n",
        "\n",
        "Given the leftward skew of our target feature, we can log-linearize the variable in order to obtain a more Gaussian distribution of ``market_value``, which is commonly recommended as a means to improve algorithmic performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Jd_SlZHopfM",
        "colab": {}
      },
      "source": [
        "sns.distplot(np.log(df['market_value']));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a5s5juMyopfR"
      },
      "source": [
        "## Evolution of Market Value over Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B3Q_yUBVopfS",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.title('Evolution of Jazz Album Mean Market Value over 1934-2020')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('2020 Mean Market Value in USD')\n",
        "plt.plot(df.groupby(by='year')['market_value'].mean())\n",
        "mean_value = df.groupby(by='year')['market_value'].mean()\n",
        "std_error = df.groupby(by='year')['market_value'].std()\n",
        "plt.fill_between(std_error.index, mean_value-std_error, mean_value+std_error, alpha=0.25,color='red')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LEBO4SQTopfY"
      },
      "source": [
        "From the above, we see that even after removing the most offending outliers, there is still tremendous variation in the price of Jazz records, which is particularly large in the mid 20th century, and slowly reduces over time, but never to a very small margin.\n",
        "\n",
        "Let us take a closer look into the breakdown of this progression in ``market_value`` over the years"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA0k5WPaGLPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.groupby('year')['market_value'].describe().head(25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBo46g2uGLPd",
        "colab_type": "text"
      },
      "source": [
        "The data extract above provides a more detailed look at the development of ``market_value`` and the number of Jazz Albums released per year over the considered time period. As we saw in the previous visualization, the mean of ``market_value`` starts very low, and gradually rises to a level that it remains fairly constant at in the late 1940s, which is between around 15 and 20 USD for a Jazz album.\n",
        "\n",
        "Given the more fine-grained look at the development of ``market_value`` and the very low number of entries per year towards the start of the time period considered in this project, we will choose to disregard those albums released before 1950, as there are relatively few releases from this period, and it is unlikely that the information learned from this era will generalize well to the time period in which most Jazz albums have been released, which begins around 1950."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndPPFxaoGLPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "record_reduction_pre_1950 = len(df)-len(df[df['year']>1949])\n",
        "print('Number of rows removed by changing focus to years after 1950: {} ({}%)'.format(record_reduction_pre_1950,np.round(100*record_reduction_pre_1950/len(df),decimals=2)))\n",
        "print('Number of columns removed by changing focus to years after 1950: {}'.format(1951-1934))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKyExqm_GLPg",
        "colab_type": "text"
      },
      "source": [
        "The above results in a minor reduction in the cardinality of the model, around 0.18% of the total data, while reducing the number of additional indicator variables, and the corresponding dimensionality required by the model, by 17 features, as the one-hot encoding we will later apply to the ``year`` feature will result in less columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9cvlUEEGLPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['year'].apply(lambda x: x > 1950)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C6fRtyVvopfZ"
      },
      "source": [
        "## Correlations \n",
        "As a next step, we will visualize the correlations between the core non-categorical attributes and the ``market_value`` target feature, in order to get an understanding of how strong the relationship between our predictor and target features are. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gpFaMcItopfZ",
        "colab": {}
      },
      "source": [
        "pairplot_columns = list(filter(lambda x: df[x].dtype in (float,int) and x not in ['master_id','release_id'],column_store._rest))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rq6c4Boeopfe",
        "colab": {}
      },
      "source": [
        "def plot_correlation_subplots(df, var, var_list,nrows,ncols,figsize=(60,60)):\n",
        "    \n",
        "    fig, ax = plt.subplots(nrows=nrows,ncols=ncols,figsize=figsize)\n",
        "\n",
        "    for row_idx, row in enumerate(ax):\n",
        "        for col_idx, col in enumerate(row):\n",
        "            row_modifier = len(var_list)-ncols*row_idx\n",
        "            list_idx = len(var_list)-row_modifier+col_idx\n",
        "            col_column = var_list[list_idx]\n",
        "            col.set_title(col_column,fontdict={'fontsize':50})\n",
        "            col.scatter(df[col_column],df[var],edgecolor='white')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vtikNcAAopfi",
        "colab": {}
      },
      "source": [
        "plot_correlation_subplots(df,'market_value',pairplot_columns,4,4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TSovDF6Uopfo"
      },
      "source": [
        "As we can see from the plots above, there are not many clear relationships that can be identified between the ``market_value`` feature and the numerical features of the dataset. This implies that performance of the model will rely heavily on an efficient encoding of non-numerical features, such as ``genre``, ``style``, ``artist``` etc. in order to be able to predict the market value of a given record with any certainty.\n",
        "\n",
        "To gain an insight into how crucial the indicator variables may be in the downstream prediction task, we will calculate the correlation coefficients of each indicator with respect to the ``market_value`` feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WznHVQc4opfo",
        "colab": {}
      },
      "source": [
        "def get_correlation_series(df,correlation_column,columns=None):\n",
        "    df = df.copy()\n",
        "    \n",
        "    if not columns:\n",
        "        columns = df.columns\n",
        "    \n",
        "    return pd.Series({\n",
        "        column: np.corrcoef(\n",
        "            df[correlation_column].values,\n",
        "            df[column].values\n",
        "        )[0][1]\n",
        "        for column in columns\n",
        "    })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0hZ86aXdopfs",
        "colab": {}
      },
      "source": [
        "market_value_indicator_correlations = get_correlation_series(df,'market_value',column_store._indicator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjwqAQqiGLP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "sns.distplot(market_value_indicator_correlations)\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Correlation Coefficient b/w Indicator and market_value')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4CenNr_7opfv",
        "colab": {}
      },
      "source": [
        "market_value_indicator_correlations.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KNI80KKxopfz"
      },
      "source": [
        "From the above, it is clear that on the whole, our indicator variables are not highly correlated with ``market_value`` either. The vast majority of indicator variables seem to have almost negligible correlations with ``market_value``, closely centred around zero, with only very few instances of stronger correlations. \n",
        "\n",
        "As a next step, we'll seek to identify and rationalize the strongest correlations of the coefficients we have calculated above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ad16quzHopf0",
        "colab": {}
      },
      "source": [
        "sign = market_value_indicator_correlations.apply(lambda x: 'positive' if x >= 0 else 'negative')\n",
        "market_value_indicator_correlations = pd.DataFrame(\n",
        "    {'correlation': market_value_indicator_correlations.abs().values,'sign': sign.values},\n",
        "    index = market_value_indicator_correlations.index\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GKd3TNdbopf6",
        "colab": {}
      },
      "source": [
        "market_value_indicator_correlations.sort_values(by='correlation',ascending=False).head(25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B9d_SiUcopf9"
      },
      "source": [
        "As we can see by investigating the top correlations, albums from Japan in particular seem to be the most highly positively correlated with ``market_value``, followed by Limited Edition albums. An interesting takeaway is that albums from Europe and North America tend to be negatively correlated with price, which may be linked to their dominance in the genre and the sheer volume of albums they release. Furthermore, we see that the Hard Bop and Modal jazz styles tend to be the most positively correlated with ``market_value``, which is also understandable, given that these were the dominant styles during the modern jazz era, which to this day are styles of Jazz that continue to define the genre in the minds of the general public. \n",
        "\n",
        "Notice also, that other than Pop, genres are not highly represented in the dataset, and the style indicators which are most common throughout the top correlations are those associated with more 'pure' Jazz ('Hard Bop','Modal','Easy Listening','Free Jazz','Big Band'). We will discuss this in further detail in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Pe6u5Jcopf-"
      },
      "source": [
        "## Indicators\n",
        "Having investigated and visualized the ``market_value`` feature, we now move on to taking a closer look at the core indicator variables which we utilize to inject additional information into the machine learning models we will build shortly.\n",
        "### Genres\n",
        "The ``genre`` feature, as previously discussed, is one of the most well-behaved features of the dataset, in that it is standardized by Discogs.com and has a relatively low dimensionality after being encoded. We summarize the ``genre`` values attributed to the albums we consider in this study below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gMUGSqk6opf-",
        "colab": {}
      },
      "source": [
        "genre_sum = df[column_store._genre].sum().sort_values(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ynXAj6pyopgC",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.xlabel('Additional Genre')\n",
        "plt.ylabel('# of Albums')\n",
        "plt.xticks(rotation=30)\n",
        "plt.bar(genre_sum.index,genre_sum);\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnBspkA0GLQK",
        "colab_type": "text"
      },
      "source": [
        "As we can see, there is relatively little cross-pollination with other genres, for albums listed as Jazz albums on Discogs. This is particularly evident, when we recognize that these groupings are not mutually exclusive, as any album can have any number of genres attributed to it. To identify what proportion of the dataset is composed of 'pure' Jazz albums, we refer to the filtering of the dataset below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ngGY4gf_opgF",
        "colab": {}
      },
      "source": [
        "pure_jazz_albums = len(df[df[column_store._genre].sum(axis=1)==0])\n",
        "print('There are {} albums which exclusively list Jazz as a genre, {}% of the dataset'.format(pure_jazz_albums,round(100*pure_jazz_albums/len(df),2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU8JLRaqGLQP",
        "colab_type": "text"
      },
      "source": [
        "Given the dominance of 'pure' Jazz over any hybrid forms in the dataset, the question arises whether or not it may be more wise to focus on the prediction of the ``market_value`` for those albums that will likely offer a more homogenous subset of the rest of the data and therefore a stronger model, given that we don't have access to the albums that most often makeup the rest of the genres. We take a look specifically at the correlations between each genre occurrence and the ``market_value`` target feature below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M50hA8IGGLQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "market_value_indicator_correlations.loc[column_store._genre,:].sort_values(by='correlation',ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRzWMueOGLQi",
        "colab_type": "text"
      },
      "source": [
        "Additionally, let us observe how many ``style`` indicator columns we would be able to remove if we focused purely on Jazz albums."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6O_1jNHGLQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pure = df[df[column_store._genre].sum(axis=1)==0]\n",
        "len(column_store._style) - len(df_pure[[style for style in column_store._style if df_pure[style].sum()==0]].columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejkdgy93GLQl",
        "colab_type": "text"
      },
      "source": [
        "The substantial decrease in dimensionality we can achieve by shifting our focus to 'pure' Jazz albums is non-trivial. As such, we will remove those rows with genres other than Jazz, and focus on estimating ``market_value`` purely for Jazz albums."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4cESwzbGLQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df_pure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yehKYLanGLQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop([i for i in set.union(column_store._genre,column_store._style) if df[i].sum() == 0],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_4ymUChGLQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_store.fit(df,col_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HLGvjqVuopgO"
      },
      "source": [
        "## Styles\n",
        "Next we will identify the characteristics of our ``style`` column, which we have encoded similarly to the ``genre`` column. As we have seen above, there has been a major reduction in the number of encoded ``style`` columns after having pivoted to a focus on 'pure' Jazz albums."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6RjL1fNHopgO",
        "colab": {}
      },
      "source": [
        "style_sum = df[column_store._style].sum()\n",
        "style_sum.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8bdVPv2GLQy",
        "colab_type": "text"
      },
      "source": [
        "From the above description, we see that there are 206 unique styles which can be attributed to Jazz albums, with an average incidence of around 1104 albums associated with each style across the dataset, and a maximum of 39805 albums.\n",
        "Let us gain further intuition around the distribution by plotting the incidence of the top 10% of styles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dqccfd9HopgV",
        "colab": {}
      },
      "source": [
        "style_sum_top_10 = style_sum[style_sum > style_sum.quantile(0.9)].sort_values(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y6Vdx85EopgZ",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(25,10))\n",
        "plt.xlabel('Listed Styles')\n",
        "plt.ylabel('# of Albums')\n",
        "plt.xticks(rotation=90)\n",
        "plt.bar(style_sum_top_10.index,style_sum_top_10)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnmaL8G7GLQ4",
        "colab_type": "text"
      },
      "source": [
        "The first thing we notice, is that there is an empty style indicator column, ``style_``, which is likely the result of a trailing comma throughout the dataset in listing the styles for a given album. As such, we will remove this indicator from the dataset and revisualize the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBE0EUtYGLQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop('style_',axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EqmhkktGLQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_store.fit(df,col_set)\n",
        "style_sum = df[column_store._style].sum()\n",
        "style_sum_top_10 = style_sum[style_sum > style_sum.quantile(0.9)].sort_values(ascending=False)\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.title('Incidence of the Top 10% Most Common Styles')\n",
        "plt.xlabel('Listed Styles')\n",
        "plt.ylabel('# of Albums')\n",
        "plt.xticks(rotation=90)\n",
        "plt.bar(style_sum_top_10.index,style_sum_top_10)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42e-9KVJGLQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.title('Distribution of Incidence for Style Indicators')\n",
        "plt.xlabel('Incidence')\n",
        "plt.ylabel('Proportion of Observations')\n",
        "sns.distplot(style_sum)\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1GctGF8GLRB",
        "colab_type": "text"
      },
      "source": [
        "From the visualizations above, as well as our calling of the ``.describe()`` method on ``style_sum`` we can draw the conclusion that the incidence of individual styles of jazz is highly skewed towards zero. While it does seem from our investigation of the most highly correlated features with ``market_value``, that certain style indicators (``style_Modal`` & ``style_Big Band`` for example) are among the most useful in the estimation of an album's market value, the great majority of indicators in this grouping are not highly informative, while also hugely increasing the dimensionality of our core dataset.\n",
        "\n",
        "As such, we take the approach of consolidating our indicator features for ``style``, such that the most relevant and frequently observed indicators can continue to be leveraged in the predictions of ``market_value``, while avoiding the issue of excessive upwards pressure on our data dimensionality. To do so, we use the ``IndicatorConsolidator()`` transformer, which counts the number of occurrences for the categories of a given indicator that has been encoded, and saves this information in a separated variable, while removing all those indicator features that do not meet a certain threshold criteria. In the implementation used below, we define an incidence minimum for encoded indicator columns we wish to retain for a given encoded feature, and replace the rest with a ``indicator_Other`` dummy feature, which is equal to 1 if any other style is attributed to the album. The transformer also introduces a ``counter_style`` feature that tracks the stylistic heterogeneity of a given record."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xojVQH4pGLRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_consolidator = IndicatorConsolidator(columns=column_store._style,\n",
        "                                           output_column='style_Other',\n",
        "                                           threshold=int(0.1*len(df)),\n",
        "                                           counter_name='counter_style')                                         \n",
        "                                           "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSsxJLypGLRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_consolidator.fit_transform(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej7G8B_uGLRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Consolidation of style results in a reduction of dimensionality by: ', len(df.columns)-len(style_consolidator.fit_transform(df).columns))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ag2mPWvlopgn"
      },
      "source": [
        "## Format Description\n",
        "As we will see, the encoding of the ``format_description`` column suffers from a similar issue as the ``style`` column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uws65qn6opgp",
        "colab": {}
      },
      "source": [
        "format_description_sum = df[column_store._format_description].sum().sort_values(ascending=False)\n",
        "format_description_sum.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neeoYKJdGLRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "format_description_sum_top_10 = format_description_sum[format_description_sum > format_description_sum.quantile(0.9)].sort_values(ascending=False)\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.title('Incidence of the Top 10% Most Common Styles')\n",
        "plt.xlabel('Listed Styles')\n",
        "plt.ylabel('# of Albums')\n",
        "plt.xticks(rotation=90)\n",
        "plt.bar(format_description_sum_top_10.index,format_description_sum_top_10)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ciRhEvdGLRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.title('Distribution of Incidence for Format Description Indicators')\n",
        "plt.xlabel('Incidence')\n",
        "plt.ylabel('Proportion of Observations')\n",
        "sns.distplot(format_description_sum)\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mUgwApCGLRP",
        "colab_type": "text"
      },
      "source": [
        "As previously mentioned, the ``format_description`` feature suffers from a leftward skew in terms of distribution in a similar manner as the ``style`` feature, with an even stronger skew, as the ``album`` and ``LP`` descriptions have a very high proportion of incidence across the dataset. As such, we will apply the same consolidation procedure of the feature as in the ``style`` case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4DPGyWbMophA",
        "colab": {}
      },
      "source": [
        "description_consolidator = IndicatorConsolidator(columns=column_store._format_description,\n",
        "                                                 output_column='format_description_Other',\n",
        "                                                 threshold=int(0.1*len(df)),\n",
        "                                                 counter_name='counter_format_description')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5iTGO-wKophE",
        "colab": {}
      },
      "source": [
        "description_consolidator.fit_transform(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFbnvvoVGLRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Consolidation of style results in a reduction of dimensionality by: ', len(df.columns)-len(description_consolidator.fit_transform(df).columns))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tmygW_biophH"
      },
      "source": [
        "## Format Name\n",
        "We apply a similar processing step to the ``format_name`` feature, which differs slightly from ``style`` and ``format_description``, in that it does not have multiple values ascribed to a given entry. As such, this feature was simply one-hot encoded. Let us see if the complete encoding of this feature makes sense, or whether we can consolidate it further and reduce the dimensionality of our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OGauOQVXophH",
        "colab": {}
      },
      "source": [
        "format_name_sum = pd.get_dummies(df['format_name']).sum().sort_values(ascending=False)\n",
        "format_name_sum.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s_Se_WvZophK",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(25,10))\n",
        "plt.xlabel('Format Names')\n",
        "plt.ylabel('# of Albums')\n",
        "plt.xticks(rotation=90)\n",
        "plt.bar(format_name_sum.index,format_name_sum)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lacNJ3rVophN"
      },
      "source": [
        "From the above, we can see that it would be wise to reduce the ``format_name`` indicator variable dimensionality, as there are only three non-trivially large format categories, namely ``CD``, ``Vinyl`` and ``Cassette``. The rest we will combine into an ``Other`` indicator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AS1vpY1oophO",
        "colab": {}
      },
      "source": [
        "format_name_consolidator = IndicatorConsolidator(output_column='format_name_other',threshold=5000)\n",
        "format_name_consolidator.fit_transform(pd.get_dummies(df['format_name']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4V08yGfEophS"
      },
      "source": [
        "## Mapping Most Albums\n",
        "In order to obtain a more spatial understanding of the distribution of Jazz albums, we visualize the releases with respect to each country in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "la292RwmophT",
        "colab": {}
      },
      "source": [
        "map_df = gpd.read_file(os.path.join(DATA_PATH,'countries/ne_110m_admin_0_countries.shp'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AR8OJFK9ophW",
        "colab": {}
      },
      "source": [
        "#Specifying the countries to be visualized by filtering out countries with no direct modern equivalent in UN encoding\n",
        "visualization_countries = list(filter(lambda x: x not in ['country_yugoslavia','country_ussr','country_taiwan'],column_store._geography_country))\n",
        "country_album_count = df[visualization_countries].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uekOwdhPophc",
        "colab": {}
      },
      "source": [
        "def assign_country_codes(country):\n",
        "    try:\n",
        "        return COUNTRY_CODES[country.split('_')[-1]]\n",
        "    except KeyError as e:\n",
        "        return '000'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J18mANJlophl",
        "colab": {}
      },
      "source": [
        "country_codes = pd.Series(tuple(map(assign_country_codes,visualization_countries)),index=visualization_countries)\n",
        "country_df = pd.DataFrame(country_album_count)\n",
        "country_df['codes'] = country_codes\n",
        "country_df['ISO_A3'] = country_df.loc[:,'codes'].map(M49_TO_ISO3) \n",
        "country_df.sort_values(0,ascending=False).head(25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd8v_B4PGLRq",
        "colab_type": "text"
      },
      "source": [
        "As we can see from the above, the great majority of albums come from the USA, Japan and Germany, with a very long tail in album releases from areas from the rest of the world. To get a better understanding of the scale, we will plot this data geospatially"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E0rwj1wSophx",
        "colab": {}
      },
      "source": [
        "map_country_df = map_df.set_index('ISO_A3').join(country_df.set_index('ISO_A3'))\n",
        "#Filling countries without values with 0 albums\n",
        "map_country_df.fillna(0,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NZZ0SuJoph1",
        "colab": {}
      },
      "source": [
        "#Plotting the Map Visualization\n",
        "def plot_map(df,column):\n",
        "    vmin, vmax = min(df[column]),round(max(df[column]),-4)\n",
        "    fig, ax = plt.subplots(1,figsize=(20,20))\n",
        "    df.plot(column=column,cmap='tab20',linewidth=0.8,ax=ax,edgecolor='0.8')\n",
        "    ax.axis('off')\n",
        "    sm = plt.cm.ScalarMappable(cmap='tab20',norm=plt.Normalize(vmin=vmin,vmax=vmax))\n",
        "    sm._a = []\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    cbar = fig.colorbar(sm,cax=cax)\n",
        "    plt.show()\n",
        "plot_map(map_country_df,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0clbOECGLRv",
        "colab_type": "text"
      },
      "source": [
        "As alluded to previously,the majority of Jazz album releases come from North America (particularly the USA), Japan and Europe (particularly Germany and the UK). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5KgG8UwCoph4"
      },
      "source": [
        "# High Level Features\n",
        "In order to proxy for the visual aspect related to purchasing albums, we use a pre-trained MobileNetV2 model in order to extract high level features from the cover images scraped from Discogs.com. With this we hope to identify whether or not there is any useful information in the visual representation of a given album in the prediction of its market value, and if visual clusters can be drawn related to this target feature.\n",
        "## Loading and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mawl6Unxoph4",
        "colab": {}
      },
      "source": [
        "with np.load(os.path.join(DATA_PATH,'high_level_features_labelled.npz')) as data:\n",
        "    image_embedding_df = pd.concat([pd.DataFrame(data[section]) for section in ('release_id','bitmap','features')],axis=1)\n",
        "    image_embedding_df.columns = ['release_id', 'bitmap'] + ['feature_%s' % i for i in range(1,1281)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MvRnb99Foph7",
        "colab": {}
      },
      "source": [
        "df = df.merge(image_embedding_df,on='release_id',how='inner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jWS-fGRWoph9",
        "colab": {}
      },
      "source": [
        "del image_embedding_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xf5ZRlbvopiA",
        "colab": {}
      },
      "source": [
        "col_set['image_embedding'] = 'feature_'\n",
        "column_store.fit(df,col_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vNb-CwjP3iTS",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from cuml import UMAP\n",
        "    dim_red_2d = UMAP(n_components=2)\n",
        "    dim_red_3d = UMAP(n_components=3)\n",
        "except:\n",
        "    from sklearn.decomposition import PCA\n",
        "    dim_red_2d = PCA(n_components=2)\n",
        "    dim_red_3d = PCA(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pWjg-xrrZBPA",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()\n",
        "image_embeddings_scaled = scaler.fit_transform(df.loc[:,sorted(list(column_store._image_embedding))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v6WcM9iSopiC",
        "colab": {}
      },
      "source": [
        "#Defining PCA and UMAP transformers\n",
        "pca_2d = PCA(n_components=2)\n",
        "umap_2d = UMAP(n_components=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XuuUjSmcopiH",
        "colab": {}
      },
      "source": [
        "pca_2d_output = pca_2d.fit_transform(image_embeddings_scaled)\n",
        "umap_2d_output = umap_2d.fit_transform(image_embeddings_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XOHWU79zbk4n",
        "colab": {}
      },
      "source": [
        "for embedding in ('pca_2d','umap_2d'):\n",
        "    col_set[embedding] = ''.join([embedding,'_'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PjwLYiynopik",
        "colab": {}
      },
      "source": [
        "df = pd.concat(\n",
        "    [df,\n",
        "     pd.DataFrame(pca_2d_output,columns=('pca_2d_0','pca_2d_1')),\n",
        "     pd.DataFrame(umap_2d_output,columns=('umap_2d_0','umap_2d_1')),\n",
        "    ],\n",
        "    axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VBcTIUaDcBiP",
        "colab": {}
      },
      "source": [
        "column_store.fit(df,col_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yfzRAOKLopim",
        "colab": {}
      },
      "source": [
        "def get_cmap(n, name='hsv'):\n",
        "    return plt.cm.get_cmap(name, n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o90aCWzbopis",
        "colab": {}
      },
      "source": [
        "def plot_indicator_2d(df, columns,embedding_columns=None,**kwargs):\n",
        "    df = df.copy()\n",
        "    if not embedding_columns:\n",
        "        embedding_columns = list(filter(lambda x: 'embedding_2d' in x,df.columns))\n",
        "    cmap = get_cmap(len(columns))\n",
        "    plt.figure(figsize=(10,10))\n",
        "    \n",
        "    if kwargs.get('colors'):\n",
        "        colors = kwargs.get('colors')\n",
        "    else:\n",
        "        cmap = get_cmap(len(columns))\n",
        "        colors = [cmap(index) for index in range(len(columns))]  \n",
        "    \n",
        "    for index,column in enumerate(columns):\n",
        "        column_embedding = df[df[column]==1][embedding_columns]\n",
        "        plt.scatter(\n",
        "            column_embedding.iloc[:,0],\n",
        "            column_embedding.iloc[:,1],\n",
        "            label=column.split('_')[-1],\n",
        "            color=colors[index],\n",
        "            alpha=0.25,\n",
        "            edgecolor='white'\n",
        "        )\n",
        "    \n",
        "    plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8cZNY7RHopiv",
        "colab": {}
      },
      "source": [
        "plot_indicator_2d(df,column_store._geography_superregion,column_store._pca_2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KtDeV0VblIzi",
        "colab": {}
      },
      "source": [
        "plot_indicator_2d(df,column_store._geography_superregion,column_store._umap_2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eTG0ZzLQhAfV",
        "colab": {}
      },
      "source": [
        "outlier_remover = OutlierRemover(features=column_store._umap_2d)\n",
        "plot_indicator_2d(outlier_remover.fit_transform(df),column_store._geography_superregion,column_store._umap_2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_bPCLIRVopix",
        "colab": {}
      },
      "source": [
        "df['market_value'].quantile([0.25,0.5,0.75,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n-WNDQNdopi0",
        "colab": {}
      },
      "source": [
        "def identify_quantile(x,lower,upper):\n",
        "    if x >= lower and x < upper:\n",
        "        return 1\n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n6ofEK3gopi2",
        "colab": {}
      },
      "source": [
        "for quantile in [0.25,0.5,0.75,1]:\n",
        "    df['market_value_quantile_%s' % quantile] = df['market_value'].apply(identify_quantile,lower=df['market_value'].quantile(quantile-0.25),upper=df['market_value'].quantile(quantile))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "avVK2_AIopi3",
        "colab": {}
      },
      "source": [
        "market_value_quantiles = list(filter(lambda x: 'market_value_quantile' in x,df.columns))\n",
        "df[market_value_quantiles].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v-EWxqfKmUAZ",
        "colab": {}
      },
      "source": [
        "plot_indicator_2d(df,market_value_quantiles,column_store._pca_2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b4iarW7WmT6Y",
        "colab": {}
      },
      "source": [
        "plot_indicator_2d(outlier_remover.fit_transform(df),market_value_quantiles,column_store._umap_2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YnyWyB5EmT09",
        "colab": {}
      },
      "source": [
        "plot_indicator_2d(df,column_store._genre,column_store._pca_2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zl2zcjMPmTvD",
        "colab": {}
      },
      "source": [
        "plot_indicator_2d(outlier_remover.fit_transform(df),column_store._genre,column_store._umap_2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1MwDjDOFmTmh",
        "colab": {}
      },
      "source": [
        "plot_indicator_2d(df,column_store._timeperiod_era,column_store._pca_2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZtQinwZmnZ35",
        "colab": {}
      },
      "source": [
        "plot_indicator_2d(outlier_remover.fit_transform(df),column_store._timeperiod_era,column_store._umap_2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lrxk3snVnZyZ",
        "colab": {}
      },
      "source": [
        "plot_indicator_2d(df,column_store._timeperiod_period,column_store._pca_2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yqTD8mz1nZtr",
        "colab": {}
      },
      "source": [
        "plot_indicator_2d(outlier_remover.fit_transform(df),column_store._timeperiod_period,column_store._umap_2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wn79XnBqorDZ"
      },
      "source": [
        "From the above visualizations of the high-level features obtained from the pre-trained computer vision model, we see that there are no clear lines to draw among the dimensionally reduced embeddings according to the indicators we are using in this analysis. While this is unfortunate, it is not completely unexpected, as cover images can vary essentially limitlessly and as such grouping them together according to our indicators is a difficult task to say the least. Nonetheless, we'll use these embeddings as input features for our machine learning models, after reducing the dimensions to 10. For this purpose we will use UMAP, as it seems to perform better in the 2d visualization scenario, being able to separate data points more effectively, and in less of a jammed together fashion as in the PCA examples."
      ]
    }
  ]
}